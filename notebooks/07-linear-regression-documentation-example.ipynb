{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLib\n",
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/02 11:49:35 WARN Utils: Your hostname, Diego-desktop resolves to a loopback address: 127.0.1.1; using 172.27.76.109 instead (on interface eth0)\n",
      "23/11/02 11:49:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/02 11:49:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "spark = SparkSession.builder.appName('lr_exemp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 11:49:39 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = spark.read.format('libsvm').load('../data/sample_linear_regression_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "| -7.896274316726144|(10,[0,1,2,3,4,5,...|\n",
      "| -8.464803554195287|(10,[0,1,2,3,4,5,...|\n",
      "| 2.1214592666251364|(10,[0,1,2,3,4,5,...|\n",
      "| 1.0720117616524107|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -5.082010756207233|(10,[0,1,2,3,4,5,...|\n",
      "|  7.887786536531237|(10,[0,1,2,3,4,5,...|\n",
      "| 14.323146365332388|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-0.8995693247765151|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|  5.601801561245534|(10,[0,1,2,3,4,5,...|\n",
      "|-3.2256352187273354|(10,[0,1,2,3,4,5,...|\n",
      "| 1.5299675726687754|(10,[0,1,2,3,4,5,...|\n",
      "| -0.250102447941961|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredictionCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxIter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mregParam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0melasticNetParam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-06\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfitIntercept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstandardization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweightCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maggregationDepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'squaredError'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxBlockSizeInMB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Linear regression.\n",
      "\n",
      "The learning objective is to minimize the specified loss function, with regularization.\n",
      "This supports two kinds of loss:\n",
      "\n",
      "* squaredError (a.k.a squared loss)\n",
      "* huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
      "\n",
      "This supports multiple types of regularization:\n",
      "\n",
      "* none (a.k.a. ordinary least squares)\n",
      "* L2 (ridge regression)\n",
      "* L1 (Lasso)\n",
      "* L2 + L1 (elastic net)\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Fitting with huber loss only supports none and L2 regularization.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.ml.linalg import Vectors\n",
      ">>> df = spark.createDataFrame([\n",
      "...     (1.0, 2.0, Vectors.dense(1.0)),\n",
      "...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      ">>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
      ">>> lr.setMaxIter(5)\n",
      "LinearRegression...\n",
      ">>> lr.getMaxIter()\n",
      "5\n",
      ">>> lr.setRegParam(0.1)\n",
      "LinearRegression...\n",
      ">>> lr.getRegParam()\n",
      "0.1\n",
      ">>> lr.setRegParam(0.0)\n",
      "LinearRegression...\n",
      ">>> model = lr.fit(df)\n",
      ">>> model.setFeaturesCol(\"features\")\n",
      "LinearRegressionModel...\n",
      ">>> model.setPredictionCol(\"newPrediction\")\n",
      "LinearRegressionModel...\n",
      ">>> model.getMaxIter()\n",
      "5\n",
      ">>> model.getMaxBlockSizeInMB()\n",
      "0.0\n",
      ">>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      ">>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
      "True\n",
      ">>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
      "True\n",
      ">>> abs(model.coefficients[0] - 1.0) < 0.001\n",
      "True\n",
      ">>> abs(model.intercept - 0.0) < 0.001\n",
      "True\n",
      ">>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      ">>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
      "True\n",
      ">>> lr.setParams(featuresCol=\"vector\")\n",
      "LinearRegression...\n",
      ">>> lr_path = temp_path + \"/lr\"\n",
      ">>> lr.save(lr_path)\n",
      ">>> lr2 = LinearRegression.load(lr_path)\n",
      ">>> lr2.getMaxIter()\n",
      "5\n",
      ">>> model_path = temp_path + \"/lr_model\"\n",
      ">>> model.save(model_path)\n",
      ">>> model2 = LinearRegressionModel.load(model_path)\n",
      ">>> model.coefficients[0] == model2.coefficients[0]\n",
      "True\n",
      ">>> model.intercept == model2.intercept\n",
      "True\n",
      ">>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      "True\n",
      ">>> model.numFeatures\n",
      "1\n",
      ">>> model.write().format(\"pmml\").save(model_path + \"_2\")\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/workspace/curso-spark/venv/lib/python3.10/site-packages/pyspark/ml/regression.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 11:49:45 WARN Instrumentation: [2734c947] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "lr_model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0073, 0.8314, -0.8095, 2.4412, 0.5192, 1.1535, -0.2989, -0.5129, -0.6197, 0.6956])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14228558260358093"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.16309157133015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027839179518600154"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 11:58:51 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('libsvm').load('../data/sample_linear_regression_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Randomly splits this :class:`DataFrame` with the provided weights.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "weights : list\n",
      "    list of doubles as weights with which to split the :class:`DataFrame`.\n",
      "    Weights will be normalized if they don't sum up to 1.0.\n",
      "seed : int, optional\n",
      "    The seed for sampling.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "list\n",
      "    List of DataFrames.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.sql import Row\n",
      ">>> df = spark.createDataFrame([\n",
      "...     Row(age=10, height=80, name=\"Alice\"),\n",
      "...     Row(age=5, height=None, name=\"Bob\"),\n",
      "...     Row(age=None, height=None, name=\"Tom\"),\n",
      "...     Row(age=None, height=None, name=None),\n",
      "... ])\n",
      "\n",
      ">>> splits = df.randomSplit([1.0, 2.0], 24)\n",
      ">>> splits[0].count()\n",
      "2\n",
      ">>> splits[1].count()\n",
      "2\n",
      "\u001b[0;31mFile:\u001b[0m      ~/workspace/curso-spark/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "df.randomSplit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_object = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[label: double, features: vector],\n",
       " DataFrame[label: double, features: vector]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|               label|\n",
      "+-------+--------------------+\n",
      "|  count|                 376|\n",
      "|   mean|-0.01589701177733...|\n",
      "| stddev|  10.123678170232827|\n",
      "|    min| -28.046018037776633|\n",
      "|    max|   27.78383192005107|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                125|\n",
      "|   mean|  1.077428610783744|\n",
      "| stddev| 10.882029949374234|\n",
      "|    min|-28.571478869743427|\n",
      "|    max|  22.31738046492344|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/02 12:01:59 WARN Instrumentation: [fdda0d83] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "val_model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = val_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -27.05853967493126|\n",
      "|-27.361257567267263|\n",
      "|-20.540761016543467|\n",
      "|-19.880293272848917|\n",
      "|-19.337826971431674|\n",
      "|-17.347917226877406|\n",
      "|-18.317221911284264|\n",
      "| -17.77359508299651|\n",
      "|-18.637636345702145|\n",
      "|-15.008971208748092|\n",
      "|-15.615317845659261|\n",
      "|-15.018646003168827|\n",
      "|-13.243891143297034|\n",
      "|-14.679926713063544|\n",
      "| -14.18139881677944|\n",
      "| -16.75392956670523|\n",
      "|-13.469233475465135|\n",
      "|-12.742055435632691|\n",
      "|  -8.21786902417424|\n",
      "| -12.75989919584323|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.738349115407024"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = test.select('features')\n",
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...| -1.5129391948121673|\n",
      "|(10,[0,1,2,3,4,5,...|  0.5557741387841906|\n",
      "|(10,[0,1,2,3,4,5,...| -2.9701230737795052|\n",
      "|(10,[0,1,2,3,4,5,...| -2.9571671440704232|\n",
      "|(10,[0,1,2,3,4,5,...|-0.32949164394004443|\n",
      "|(10,[0,1,2,3,4,5,...| -1.8203753960863536|\n",
      "|(10,[0,1,2,3,4,5,...| 0.04200834527963282|\n",
      "|(10,[0,1,2,3,4,5,...|  0.7081954571204971|\n",
      "|(10,[0,1,2,3,4,5,...|  2.3762060702494128|\n",
      "|(10,[0,1,2,3,4,5,...| -1.0766878322733984|\n",
      "|(10,[0,1,2,3,4,5,...|-0.11677042657998477|\n",
      "|(10,[0,1,2,3,4,5,...|-0.41873879026239086|\n",
      "|(10,[0,1,2,3,4,5,...| -2.0670894461192537|\n",
      "|(10,[0,1,2,3,4,5,...|-0.14222619668764447|\n",
      "|(10,[0,1,2,3,4,5,...| -0.5813594361516858|\n",
      "|(10,[0,1,2,3,4,5,...|   2.424951057629789|\n",
      "|(10,[0,1,2,3,4,5,...|  -0.506897455687568|\n",
      "|(10,[0,1,2,3,4,5,...| 0.26277522418119453|\n",
      "|(10,[0,1,2,3,4,5,...| -3.4226806537145857|\n",
      "|(10,[0,1,2,3,4,5,...|   1.431483259065449|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = val_model.transform(unlabeled_data)\n",
    "pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
