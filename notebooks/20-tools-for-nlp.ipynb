{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "## Tools for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style='darkgrid')\n",
    "sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName('nlp') \\\n",
    "    #.setMaster('local') \\\n",
    "    #.set('spark.executor.memory', '8g') \\\n",
    "    #.set('spark.driver.maxResultSize', '8g') \\\n",
    "    #.set(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    #.set(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    #.set(\"spark.sql.shuffle.partitions\", \"5\") \\\n",
    "    #.set(\"spark.memory.offHeap.enabled\", \"false\") \\\n",
    "    #.set(\"spark.reducer.maxSizeInFlight\", \"96m\") \\\n",
    "    #.set(\"spark.shuffle.file.buffer\", \"256k\") \\\n",
    "    #.set(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "    #.set('spark.sql.autoBroadcastJoinThreshold', '-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_df = spark.createDataFrame(\n",
    "    [\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,regression,models,are,neat')\n",
    "    ],\n",
    "    ['id', 'sentence']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A tokenizer that converts the input string to lowercase and then\n",
      "splits it by white spaces.\n",
      "\n",
      ".. versionadded:: 1.3.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([(\"a b c\",)], [\"text\"])\n",
      ">>> tokenizer = Tokenizer(outputCol=\"words\")\n",
      ">>> tokenizer.setInputCol(\"text\")\n",
      "Tokenizer...\n",
      ">>> tokenizer.transform(df).head()\n",
      "Row(text='a b c', words=['a', 'b', 'c'])\n",
      ">>> # Change a parameter.\n",
      ">>> tokenizer.setParams(outputCol=\"tokens\").transform(df).head()\n",
      "Row(text='a b c', tokens=['a', 'b', 'c'])\n",
      ">>> # Temporarily modify a parameter.\n",
      ">>> tokenizer.transform(df, {tokenizer.outputCol: \"words\"}).head()\n",
      "Row(text='a b c', words=['a', 'b', 'c'])\n",
      ">>> tokenizer.transform(df).head()\n",
      "Row(text='a b c', tokens=['a', 'b', 'c'])\n",
      ">>> # Must use keyword arguments to specify params.\n",
      ">>> tokenizer.setParams(\"text\")\n",
      "Traceback (most recent call last):\n",
      "    ...\n",
      "TypeError: Method setParams forces keyword arguments.\n",
      ">>> tokenizerPath = temp_path + \"/tokenizer\"\n",
      ">>> tokenizer.save(tokenizerPath)\n",
      ">>> loadedTokenizer = Tokenizer.load(tokenizerPath)\n",
      ">>> loadedTokenizer.transform(df).head().tokens == tokenizer.transform(df).head().tokens\n",
      "True\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, inputCol=None, outputCol=None)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/ml/feature.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    inputCol='sentence',\n",
    "    outputCol='words'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRegexTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mminTokenLength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgaps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpattern\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\\\s+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoLowercase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A regex based tokenizer that extracts tokens either by using the\n",
      "provided regex pattern (in Java dialect) to split the text\n",
      "(default) or repeatedly matching the regex (if gaps is false).\n",
      "Optional parameters also allow filtering tokens using a minimal\n",
      "length.\n",
      "It returns an array of strings that can be empty.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([(\"A B  c\",)], [\"text\"])\n",
      ">>> reTokenizer = RegexTokenizer()\n",
      ">>> reTokenizer.setInputCol(\"text\")\n",
      "RegexTokenizer...\n",
      ">>> reTokenizer.setOutputCol(\"words\")\n",
      "RegexTokenizer...\n",
      ">>> reTokenizer.transform(df).head()\n",
      "Row(text='A B  c', words=['a', 'b', 'c'])\n",
      ">>> # Change a parameter.\n",
      ">>> reTokenizer.setParams(outputCol=\"tokens\").transform(df).head()\n",
      "Row(text='A B  c', tokens=['a', 'b', 'c'])\n",
      ">>> # Temporarily modify a parameter.\n",
      ">>> reTokenizer.transform(df, {reTokenizer.outputCol: \"words\"}).head()\n",
      "Row(text='A B  c', words=['a', 'b', 'c'])\n",
      ">>> reTokenizer.transform(df).head()\n",
      "Row(text='A B  c', tokens=['a', 'b', 'c'])\n",
      ">>> # Must use keyword arguments to specify params.\n",
      ">>> reTokenizer.setParams(\"text\")\n",
      "Traceback (most recent call last):\n",
      "    ...\n",
      "TypeError: Method setParams forces keyword arguments.\n",
      ">>> regexTokenizerPath = temp_path + \"/regex-tokenizer\"\n",
      ">>> reTokenizer.save(regexTokenizerPath)\n",
      ">>> loadedReTokenizer = RegexTokenizer.load(regexTokenizerPath)\n",
      ">>> loadedReTokenizer.getMinTokenLength() == reTokenizer.getMinTokenLength()\n",
      "True\n",
      ">>> loadedReTokenizer.getGaps() == reTokenizer.getGaps()\n",
      "True\n",
      ">>> loadedReTokenizer.transform(df).take(1) == reTokenizer.transform(df).take(1)\n",
      "True\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, minTokenLength=1, gaps=True, pattern=\"\\s+\", inputCol=None,                  outputCol=None, toLowercase=True)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/ml/feature.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "RegexTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(\n",
    "    inputCol='sentence',\n",
    "    outputCol='words',\n",
    "    pattern='\\\\W'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataTypeOrString'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturnType\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DataTypeOrString'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0museArrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UserDefinedFunctionLike'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UserDefinedFunctionLike'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Creates a user defined function (UDF).\n",
      "\n",
      ".. versionadded:: 1.3.0\n",
      "\n",
      ".. versionchanged:: 3.4.0\n",
      "    Supports Spark Connect.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "f : function\n",
      "    python function if used as a standalone function\n",
      "returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "    the return type of the user-defined function. The value can be either a\n",
      "    :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "useArrow : bool or None\n",
      "    whether to use Arrow to optimize the (de)serialization. When it is None, the\n",
      "    Spark config \"spark.sql.execution.pythonUDF.arrow.enabled\" takes effect.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.sql.types import IntegerType\n",
      ">>> slen = udf(lambda s: len(s), IntegerType())\n",
      ">>> @udf\n",
      "... def to_upper(s):\n",
      "...     if s is not None:\n",
      "...         return s.upper()\n",
      "...\n",
      ">>> @udf(returnType=IntegerType())\n",
      "... def add_one(x):\n",
      "...     if x is not None:\n",
      "...         return x + 1\n",
      "...\n",
      ">>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      ">>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "+----------+--------------+------------+\n",
      "|slen(name)|to_upper(name)|add_one(age)|\n",
      "+----------+--------------+------------+\n",
      "|         8|      JOHN DOE|          22|\n",
      "+----------+--------------+------------+\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The user-defined functions are considered deterministic by default. Due to\n",
      "optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "more times than it is present in the query. If your function is not deterministic, call\n",
      "`asNondeterministic` on the user defined function. E.g.:\n",
      "\n",
      ">>> from pyspark.sql.types import IntegerType\n",
      ">>> import random\n",
      ">>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "\n",
      "The user-defined functions do not support conditional expressions or short circuiting\n",
      "in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "\n",
      "The user-defined functions do not take keyword arguments on the calling side.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/sql/functions.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "F.udf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = F.udf(lambda words : len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendized = tokenizer.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokendized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokendized.withColumn('tokens', count_tokens(F.col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg_tokenized = regex_tokenizer.transform(sen_df).withColumn('tokens', count_tokens(F.col('words')))\n",
    "rg_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                 raw|\n",
      "+---+--------------------+\n",
      "|  0|[I, saw, the, red...|\n",
      "|  1|[Mary, had, a, li...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame(\n",
    "    [\n",
    "        (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "        (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "    ],\n",
    "    [\"id\", \"raw\"])\n",
    "\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstopWords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcaseSensitive\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputCols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputCols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A feature transformer that filters out stop words from input.\n",
      "Since 3.0.0, :py:class:`StopWordsRemover` can filter out multiple columns at once by setting\n",
      "the :py:attr:`inputCols` parameter. Note that when both the :py:attr:`inputCol` and\n",
      ":py:attr:`inputCols` parameters are set, an Exception will be thrown.\n",
      "\n",
      ".. versionadded:: 1.6.0\n",
      "\n",
      "Notes\n",
      "-----\n",
      "null values from input array are preserved unless adding null to stopWords explicitly.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n",
      ">>> remover = StopWordsRemover(stopWords=[\"b\"])\n",
      ">>> remover.setInputCol(\"text\")\n",
      "StopWordsRemover...\n",
      ">>> remover.setOutputCol(\"words\")\n",
      "StopWordsRemover...\n",
      ">>> remover.transform(df).head().words == ['a', 'c']\n",
      "True\n",
      ">>> stopWordsRemoverPath = temp_path + \"/stopwords-remover\"\n",
      ">>> remover.save(stopWordsRemoverPath)\n",
      ">>> loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n",
      ">>> loadedRemover.getStopWords() == remover.getStopWords()\n",
      "True\n",
      ">>> loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\n",
      "True\n",
      ">>> loadedRemover.transform(df).take(1) == remover.transform(df).take(1)\n",
      "True\n",
      ">>> df2 = spark.createDataFrame([([\"a\", \"b\", \"c\"], [\"a\", \"b\"])], [\"text1\", \"text2\"])\n",
      ">>> remover2 = StopWordsRemover(stopWords=[\"b\"])\n",
      ">>> remover2.setInputCols([\"text1\", \"text2\"]).setOutputCols([\"words1\", \"words2\"])\n",
      "StopWordsRemover...\n",
      ">>> remover2.transform(df2).show()\n",
      "+---------+------+------+------+\n",
      "|    text1| text2|words1|words2|\n",
      "+---------+------+------+------+\n",
      "|[a, b, c]|[a, b]|[a, c]|   [a]|\n",
      "+---------+------+------+------+\n",
      "...\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false,                  locale=None, inputCols=None, outputCols=None)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/ml/feature.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "StopWordsRemover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(\n",
    "    inputCol='raw',\n",
    "    outputCol='filtered',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                 raw|            filtered|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, red...| [saw, red, balloon]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentenceData).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mNGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A feature transformer that converts the input array of strings into an array of n-grams. Null\n",
      "values in the input array are ignored.\n",
      "It returns an array of n-grams where each n-gram is represented by a space-separated string of\n",
      "words.\n",
      "When the input is empty, an empty array is returned.\n",
      "When the input array length is less than n (number of elements per n-gram), no n-grams are\n",
      "returned.\n",
      "\n",
      ".. versionadded:: 1.5.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> df = spark.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n",
      ">>> ngram = NGram(n=2)\n",
      ">>> ngram.setInputCol(\"inputTokens\")\n",
      "NGram...\n",
      ">>> ngram.setOutputCol(\"nGrams\")\n",
      "NGram...\n",
      ">>> ngram.transform(df).head()\n",
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b', 'b c', 'c d', 'd e'])\n",
      ">>> # Change n-gram length\n",
      ">>> ngram.setParams(n=4).transform(df).head()\n",
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b c d', 'b c d e'])\n",
      ">>> # Temporarily modify output column.\n",
      ">>> ngram.transform(df, {ngram.outputCol: \"output\"}).head()\n",
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], output=['a b c d', 'b c d e'])\n",
      ">>> ngram.transform(df).head()\n",
      "Row(inputTokens=['a', 'b', 'c', 'd', 'e'], nGrams=['a b c d', 'b c d e'])\n",
      ">>> # Must use keyword arguments to specify params.\n",
      ">>> ngram.setParams(\"text\")\n",
      "Traceback (most recent call last):\n",
      "    ...\n",
      "TypeError: Method setParams forces keyword arguments.\n",
      ">>> ngramPath = temp_path + \"/ngram\"\n",
      ">>> ngram.save(ngramPath)\n",
      ">>> loadedNGram = NGram.load(ngramPath)\n",
      ">>> loadedNGram.getN() == ngram.getN()\n",
      "True\n",
      ">>> loadedNGram.transform(df).take(1) == ngram.transform(df).take(1)\n",
      "True\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, n=2, inputCol=None, outputCol=None)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/ml/feature.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "NGram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|[Hi, I, heard, ab...|\n",
      "|  1|[I, wish, Java, c...|\n",
      "|  2|[Logistic, regres...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "wordDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = NGram(\n",
    "    n=2,\n",
    "    inputCol=\"words\",\n",
    "    outputCol=\"ngrams\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame(\n",
    "    [\n",
    "    (0.0, 'Hi I heard about Spark'),\n",
    "    (0.0, 'I wish Java could use case classes'),\n",
    "    (1.0, 'Logistic regression models are neat')\n",
    "    ],\n",
    "    ['label', 'sentence']\n",
    ")\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|label|sentence                           |words                                     |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
      "+-----+-----------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data = tokenizer.transform(sentenceData)\n",
    "words_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(\n",
    "    inputCol='words',\n",
    "    outputCol='rawFeatures'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------------------------------------+\n",
      "|label|sentence                           |words                                     |rawFeatures                                                                         |\n",
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------------------------------------+\n",
      "|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |(262144,[18700,19036,33808,66273,173558],[1.0,1.0,1.0,1.0,1.0])                     |\n",
      "|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|(262144,[19036,20719,55551,58672,98717,109547,192310],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |(262144,[46243,58267,91006,160975,190884],[1.0,1.0,1.0,1.0,1.0])                    |\n",
      "+-----+-----------------------------------+------------------------------------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data = hashing_tf.transform(words_data)\n",
    "featurized_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(\n",
    "    inputCol='rawFeatures',\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf_model = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/10 14:03:22 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/11/10 14:03:23 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                      |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(262144,[18700,19036,33808,66273,173558],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n",
      "|0.0  |(262144,[19036,20719,55551,58672,98717,109547,192310],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n",
      "|1.0  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/10 14:03:23 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('label', 'features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "      (0, 'a b c'.split(' '))  ,\n",
    "      (1, 'a b b c a'.split(' ')),\n",
    "    ],\n",
    "    ['id', 'words']\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(\n",
    "    inputCol='words',\n",
    "    outputCol='features',\n",
    "    vocabSize=3,\n",
    "    minDF=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = cv.fit(df).transform(df)\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
