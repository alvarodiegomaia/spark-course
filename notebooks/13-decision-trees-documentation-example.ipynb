{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "## Decision trees\n",
    "### Documentation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style='darkgrid')\n",
    "sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 09:43:30 WARN Utils: Your hostname, Diego-desktop resolves to a loopback address: 127.0.1.1; using 172.27.76.109 instead (on interface eth0)\n",
      "23/11/07 09:43:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 09:43:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "spark = SparkSession.builder.appName('decision_trees_documentation_example').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 09:43:37 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('libsvm').load('../data/sample_libsvm_data.txt')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'label'\n",
    "\n",
    "df_zeros = df.filter(df[target] == 0)\n",
    "df_ones = df.filter(df[target] == 1)\n",
    "\n",
    "train_zeros, test_zeros = df_zeros.randomSplit([0.7, 0.3], seed=random_seed)\n",
    "train_ones, test_ones = df_ones.randomSplit([0.7, 0.3], seed=random_seed)\n",
    "\n",
    "train = train_zeros.union(train_ones)\n",
    "test = test_zeros.union(test_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabelCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredictionCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprobabilityCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'probability'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rawPrediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxDepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxBins\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mminInstancesPerNode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mminInfoGain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmaxMemoryInMB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcacheNodeIds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcheckpointInterval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mimpurity\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweightCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mleafCol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mminWeightFractionPerNode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "`Decision tree <http://en.wikipedia.org/wiki/Decision_tree_learning>`_\n",
      "learning algorithm for classification.\n",
      "It supports both binary and multiclass labels, as well as both continuous and categorical\n",
      "features.\n",
      "\n",
      ".. versionadded:: 1.4.0\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from pyspark.ml.linalg import Vectors\n",
      ">>> from pyspark.ml.feature import StringIndexer\n",
      ">>> df = spark.createDataFrame([\n",
      "...     (1.0, Vectors.dense(1.0)),\n",
      "...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n",
      ">>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
      ">>> si_model = stringIndexer.fit(df)\n",
      ">>> td = si_model.transform(df)\n",
      ">>> dt = DecisionTreeClassifier(maxDepth=2, labelCol=\"indexed\", leafCol=\"leafId\")\n",
      ">>> model = dt.fit(td)\n",
      ">>> model.getLabelCol()\n",
      "'indexed'\n",
      ">>> model.setFeaturesCol(\"features\")\n",
      "DecisionTreeClassificationModel...\n",
      ">>> model.numNodes\n",
      "3\n",
      ">>> model.depth\n",
      "1\n",
      ">>> model.featureImportances\n",
      "SparseVector(1, {0: 1.0})\n",
      ">>> model.numFeatures\n",
      "1\n",
      ">>> model.numClasses\n",
      "2\n",
      ">>> print(model.toDebugString)\n",
      "DecisionTreeClassificationModel...depth=1, numNodes=3...\n",
      ">>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      ">>> model.predict(test0.head().features)\n",
      "0.0\n",
      ">>> model.predictRaw(test0.head().features)\n",
      "DenseVector([1.0, 0.0])\n",
      ">>> model.predictProbability(test0.head().features)\n",
      "DenseVector([1.0, 0.0])\n",
      ">>> result = model.transform(test0).head()\n",
      ">>> result.prediction\n",
      "0.0\n",
      ">>> result.probability\n",
      "DenseVector([1.0, 0.0])\n",
      ">>> result.rawPrediction\n",
      "DenseVector([1.0, 0.0])\n",
      ">>> result.leafId\n",
      "0.0\n",
      ">>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      ">>> model.transform(test1).head().prediction\n",
      "1.0\n",
      ">>> dtc_path = temp_path + \"/dtc\"\n",
      ">>> dt.save(dtc_path)\n",
      ">>> dt2 = DecisionTreeClassifier.load(dtc_path)\n",
      ">>> dt2.getMaxDepth()\n",
      "2\n",
      ">>> model_path = temp_path + \"/dtc_model\"\n",
      ">>> model.save(model_path)\n",
      ">>> model2 = DecisionTreeClassificationModel.load(model_path)\n",
      ">>> model.featureImportances == model2.featureImportances\n",
      "True\n",
      ">>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      "True\n",
      ">>> df3 = spark.createDataFrame([\n",
      "...     (1.0, 0.2, Vectors.dense(1.0)),\n",
      "...     (1.0, 0.8, Vectors.dense(1.0)),\n",
      "...     (0.0, 1.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      ">>> si3 = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n",
      ">>> si_model3 = si3.fit(df3)\n",
      ">>> td3 = si_model3.transform(df3)\n",
      ">>> dt3 = DecisionTreeClassifier(maxDepth=2, weightCol=\"weight\", labelCol=\"indexed\")\n",
      ">>> model3 = dt3.fit(td3)\n",
      ">>> print(model3.toDebugString)\n",
      "DecisionTreeClassificationModel...depth=1, numNodes=3...\n",
      "\u001b[0;31mInit docstring:\u001b[0m __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                  seed=None, weightCol=None, leafCol=\"\", minWeightFractionPerNode=0.0)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pyspark/lib/python3.12/site-packages/pyspark/ml/classification.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = ['DecisionTreeClassifier', 'RandomForestClassifier', 'GBTClassifier']\n",
    "models_list = [DecisionTreeClassifier, RandomForestClassifier, GBTClassifier]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for i, model in enumerate(models_names):\n",
    "    models[model] = {\n",
    "        'model' : models_list[i](\n",
    "            labelCol='label',\n",
    "            featuresCol='features',\n",
    "            seed=random_seed\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    models[model] |= {'fit' : models[model]['model'].fit(train)}\n",
    "    \n",
    "    models[model] |= {'pred' : models[model]['fit'].transform(test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models_names:\n",
    "    models[name] |= {\n",
    "            'accuracy' : evaluatorMulti.evaluate(models[name]['pred'], {evaluatorMulti.metricName: 'accuracy'})\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models accuracy:\n",
      "\tDecisionTreeClassifier : 0.97\n",
      "\tRandomForestClassifier : 1.00\n",
      "\tGBTClassifier : 0.97\n"
     ]
    }
   ],
   "source": [
    "print('Models accuracy:')\n",
    "for name in models_names:\n",
    "    print(f'\\t{name} : {models[name]['accuracy']:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(692, {149: 0.0028, 206: 0.0208, 243: 0.0031, 262: 0.0418, 268: 0.0026, 271: 0.0026, 299: 0.005, 329: 0.0411, 346: 0.0028, 350: 0.008, 351: 0.1356, 378: 0.047, 379: 0.0419, 385: 0.0446, 399: 0.0399, 401: 0.0101, 406: 0.05, 412: 0.0114, 433: 0.0528, 434: 0.0925, 440: 0.0251, 443: 0.0089, 453: 0.0386, 455: 0.045, 483: 0.0409, 485: 0.0091, 490: 0.083, 495: 0.0292, 517: 0.0469, 518: 0.0015, 520: 0.0019, 544: 0.0028, 599: 0.0091, 628: 0.0016})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[models_names[1]]['fit'].featureImportances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
